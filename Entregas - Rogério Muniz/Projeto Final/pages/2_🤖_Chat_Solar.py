# Conte√∫do completo e corrigido para o arquivo: pages/2_ü§ñ_Chat_Solar.py

import streamlit as st
from llama_cpp import Llama
import os

# ---------- CONFIGURA√á√ÉO DA P√ÅGINA ----------
st.set_page_config(page_title="Chat Solar", page_icon="ü§ñ", layout="centered")
st.title("ü§ñ Chat Solar")
st.info("Tire suas d√∫vidas sobre energia solar com nosso assistente virtual!")

# ---------- FUN√á√ÉO DE CARREGAMENTO DO MODELO ----------
@st.cache_resource
def carregar_modelo_ia():
    nome_do_arquivo_modelo = "Meta-Llama-3-8B-Instruct.Q4_K_M.gguf"
    if os.path.exists(nome_do_arquivo_modelo):
        try:
            llm = Llama(
                model_path=nome_do_arquivo_modelo,
                n_ctx=4096,
                n_gpu_layers=0,
                n_batch=256,
                verbose=False
            )
            return llm
        except Exception as e:
            st.error(f"‚ùå Erro ao carregar o modelo: {e}")
            return None
    else:
        st.error(f"‚ùå Arquivo do modelo '{nome_do_arquivo_modelo}' N√ÉO ENCONTRADO.")
        return None

llm = carregar_modelo_ia()

# ---------- L√ìGICA DO CHATBOT COM CONTEXTO E CONHECIMENTO ESPECIALIZADO ----------

# 1. INICIALIZA√á√ÉO INTELIGENTE DO CHAT
if "messages" not in st.session_state:
    st.session_state.messages = []

     # Verifica se uma proposta foi gerada para iniciar com a mensagem proativa
    if st.session_state.get("proposta_gerada", False):
        resultados = st.session_state.resultados
        nome_cliente = resultados.get('nome_cliente', 'usu√°rio')
        payback_anos = resultados.get('payback_anos', 'N/A')

        # Mensagem proativa e personalizada
        mensagem_proativa = (
            f"Ol√°, {nome_cliente}! Bem-vindo(a) ao nosso chat. "
            f"Vi que voc√™ acabou de gerar uma simula√ß√£o para seu projeto em {resultados.get('cidade_cliente', '')} "
            f"e o resultado do payback foi de {payback_anos} anos, o que √© um √≥timo indicador! "
            f"Voc√™ tem alguma d√∫vida espec√≠fica sobre esses resultados ou sobre energia solar em geral?"
        )
        st.session_state.messages.append({"role": "assistant", "content": mensagem_proativa})
    else:
        # Mensagem padr√£o se o usu√°rio veio direto para o chat
        st.session_state.messages.append({"role": "assistant", "content": "Ol√°! Sou seu assistente solar. Como posso ajudar com suas d√∫vidas sobre energia fotovoltaica no Brasil?"})
        
# --- Base de Conhecimento Injetada ---
informacao_lei_14300 = """
- **Custo de Disponibilidade:** Todo consumidor deve pagar uma taxa m√≠nima para estar conectado √† rede, mesmo que n√£o consuma nada. Esse valor varia, mas cobre custos de infraestrutura.
- **Tarifa√ß√£o do Fio B (Lei 14.300):** Para novas conex√µes (ap√≥s jan/2023), a energia que voc√™ injeta na rede n√£o compensa 100% da energia que voc√™ consome. Uma parte dela √© usada para pagar pelo uso da infraestrutura da rede (o "Fio B"). Isso significa que, na pr√°tica, sua conta de luz n√£o ser√° zerada, mas sim drasticamente reduzida. Voc√™ ainda pagar√° uma pequena quantia referente a essa tarifa√ß√£o e ao custo de disponibilidade.
"""
concessionarias = {
    "GO": "Equatorial Goi√°s", "SP": "Enel SP / CPFL / Elektro", "RJ": "Light / Enel RJ", "MG": "CEMIG",
    "PR": "Copel", "SC": "CELESC", "RS": "CEEE / RGE", "DF": "CEB", "MT": "Energisa", "MS": "Energisa",
    # Adicione outros estados e concession√°rias conforme necess√°rio
}

# 1. CONSTRU√á√ÉO DO PROMPT DO SISTEMA
prompt_sistema_base = (
    "Voc√™ √© um assistente virtual chamado 'Chat Solar', especialista em energia solar fotovoltaica no Brasil. "
    "Suas respostas devem ser claras, objetivas e amig√°veis. "
    "Responda apenas a perguntas relacionadas a energia solar. "
    "Se o usu√°rio perguntar sobre outro assunto, recuse educadamente e diga que seu conhecimento √© focado em energia solar."
    "As respostas devem ser verdadeiras, n√£o invente informa√ß√µes e nem dados, caso o usu√°rio pe√ßa algum dado que n√£o saiba, diga ao usu√°rio entrar em contato com um dos nossos especialistas."
    "A proposta n√£o √© totalmente certa, por isso enfatize sempre que os valores s√£o estimativas e podem variar, recomende o cliente entrar em contato com um dos nossos especialistas. "
    "Lembre-se de que voc√™ esta falando como um vendedor, sua miss√£o √© ajudar o usu√°rio a entender melhor a energia solar e incentiv√°-lo a entrar em contato com um especialista para obter uma proposta personalizada."
)

prompt_sistema_com_conhecimento = prompt_sistema_base + "\n\n[INFORMA√á√ïES T√âCNICAS E LEGAIS IMPORTANTES PARA USAR NAS RESPOSTAS]\n" + informacao_lei_14300

if st.session_state.get("proposta_gerada", False):
    resultados = st.session_state.resultados
    cidade_estado = resultados.get('cidade_cliente', '')

    # Extrai a sigla do estado (ex: 'Goi√¢nia - GO' -> 'GO')
    sigla_estado = ""
    if " - " in cidade_estado:
        sigla_estado = cidade_estado.split(" - ")[-1].upper()

    nome_concessionaria = concessionarias.get(sigla_estado, "a concession√°ria local")
    
    contexto_do_projeto = (
        "\n\n[INSTRU√á√ÉO ADICIONAL]\n"
        "O usu√°rio j√° realizou uma simula√ß√£o de projeto. Use os dados do contexto abaixo para responder a perguntas sobre 'meu projeto', 'minha proposta', etc. "
        "N√£o √© necess√°rio apresentar os dados novamente, a menos que o usu√°rio pe√ßa especificamente."
        "\n\n[CONTEXTO DO PROJETO DO USU√ÅRIO]\n"
        f"- Nome do Cliente: {resultados.get('nome_cliente', 'N/A')}\n"
        f"- Cidade: {resultados.get('cidade_cliente', 'N/A')}\n"
        f"- Pot√™ncia do Sistema: {resultados.get('potencia_kWp', 'N/A')} kWp\n"
        f"- Custo Estimado: {resultados.get('preco_formatado', 'N/A')}\n"
        f"- Economia Mensal Estimada: {resultados.get('economia_formatada', 'N/A')}\n"
        f"- Retorno do Investimento: {resultados.get('payback_anos', 'N/A')} anos"
    )
    prompt_final_sistema = prompt_sistema_com_conhecimento + contexto_do_projeto
else:
    prompt_final_sistema = prompt_sistema_com_conhecimento

# 2. L√ìGICA DO CHAT (Resto do c√≥digo)
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Ol√°! Sou seu assistente solar. Como posso ajudar com suas d√∫vidas sobre energia fotovoltaica no Brasil?"}]

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt_usuario := st.chat_input("Qual sua d√∫vida?"):
    st.session_state.messages.append({"role": "user", "content": prompt_usuario})
    with st.chat_message("user"):
        st.markdown(prompt_usuario)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        with st.spinner("Consultando base de conhecimento..."):
            conversa_formatada = "".join([f"<|start_header_id|>{msg['role']}<|end_header_id|>\n\n{msg['content']}<|eot_id|>" for msg in st.session_state.messages])
            prompt_final_para_ia = (
                f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{prompt_final_sistema}<|eot_id|>"
                f"{conversa_formatada}"
                f"<|start_header_id|>assistant<|end_header_id|>\n\n"
            )

            if llm:
                try:
                    output = llm(prompt_final_para_ia, max_tokens=512, temperature=0.4, stop=["<|eot_id|>", "<|end_of_text|>", "<|user|>"], echo=False)
                    resposta_ia = output["choices"][0]["text"].strip()
                    message_placeholder.markdown(resposta_ia)
                    st.session_state.messages.append({"role": "assistant", "content": resposta_ia})
                except Exception as e:
                    st.error(f"Ocorreu um erro ao gerar a resposta: {e}")
            else:
                message_placeholder.markdown("Desculpe, meu c√©rebro de IA n√£o est√° funcionando no momento.")